{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font face='georgia'>\n",
    "    <h4><strong>Build a TFIDF Vectorizer & compare its results with Sklearn:</strong></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ```fit``` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "\n",
    "vocab = fit(corpus)\n",
    "print(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing sklearn tfid vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Output from custom fit method and output from sklearn tfidf vectorizer fit method are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating custom ```idf```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "def idf_(dataset, vocab):\n",
    "    unique_words = list(vocab.keys())\n",
    "    # intializing a dict to store idf values as values and subsequent words as keys\n",
    "    idf_val = {}\n",
    "    for word in unique_words: # for every word in list of unique words\n",
    "        doc_with_words = 0\n",
    "        for row in dataset: # for each sentence in dataset\n",
    "            if word in row.split():\n",
    "                doc_with_words += 1\n",
    "        # idf calculation\n",
    "        idf_val[word] = 1 + math.log((1 + len(dataset)) / float(1 + doc_with_words))\n",
    "    return np.array(list(idf_val.values()))\n",
    "\n",
    "print(idf_(corpus, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : `idf` from custom method and `idf` from sklear tfidf vectorizer are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ```transform``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset, vocab):\n",
    "    \n",
    "    # intializing required list and dictionaries for calculating tf and idf\n",
    "    unique_words = list(vocab.keys())\n",
    "    idf_val = {}\n",
    "    tf_val = {}\n",
    "    tf_idf_values = []\n",
    "    if isinstance(dataset, (list,)):  # check if its list type or not\n",
    "        \n",
    "        # calculating idf\n",
    "        for word in unique_words: # for each word in list of unique words\n",
    "            doc_with_words = 0\n",
    "            for row in dataset:   # for each sentence in dataset\n",
    "                if word in row.split():\n",
    "                    doc_with_words += 1\n",
    "            # idf calculation\n",
    "            idf_val[word] = 1 + math.log((1 + len(dataset)) / float(1 + doc_with_words))  \n",
    "         \n",
    "        # calculating tf\n",
    "        for word in unique_words: # for each word in list of unique words\n",
    "            sent_tf_vec = []\n",
    "            for row in dataset: # for each sentence in dataset\n",
    "                doc_freq = 0\n",
    "                for token in row.split(): # for each word in the sentence\n",
    "                    if word == token:\n",
    "                        doc_freq += 1\n",
    "                tf = doc_freq / len(row.split()) # tf calculation\n",
    "                sent_tf_vec.append(tf)\n",
    "            tf_val[word] = sent_tf_vec\n",
    "        \n",
    "        \n",
    "        # calculating tfidf values\n",
    "        for token in tqdm(tf_val.keys()):  # for each word in tf-dictionary\n",
    "            tf_idf_sentences = []           \n",
    "            for tf_sent in tf_val[token]: \n",
    "                tf_idf_val = tf_sent * idf_val[token]\n",
    "                tf_idf_sentences.append(tf_idf_val)\n",
    "            tf_idf_values.append(tf_idf_sentences)\n",
    "        # normalizing the final output and converting it into a sparse matrix\n",
    "        return csr_matrix(normalize(np.array(tf_idf_values).T, norm = 'l2'))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 9/9 [00:00<00:00, 9004.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 9/9 [00:00<00:00, 8989.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 9/9 [00:00<00:00, 8992.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# shape of custom tfidf vectorizer after apllying transform method\n",
    "print(transform(corpus, vocab).shape)\n",
    "# sparse matrix output of first sentence\n",
    "print(transform(corpus, vocab)[0])\n",
    "# dense matrix output of first sentence\n",
    "print(transform(corpus, vocab)[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "print(skl_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sparse matrix output of  sklear tfidf vectorizer of first sentence\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# dense matrix output of sklearn tfidf vectorizer of first sentence\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: final output of custom transform method and sklearn tdidf transform method are same. Though there are  difference in ordering of the sparse output , but values are same ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<font face='georgia'>\n",
    "    <h4><strong>Build a custom TFIDF Vectorizer & compare its results with Sklearn with max feature functionality:</strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    strings = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ```fit``` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "        \n",
    "vocabulary = fit(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating custom ```idf``` with max features functionality\n",
    "This function return a dictionary with top 50 idf scores as values and subsequent words as keys. We can put different values of max features if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aailiyah': 6.922918004572872,\n",
       " 'abandoned': 6.922918004572872,\n",
       " 'abroad': 6.922918004572872,\n",
       " 'abstruse': 6.922918004572872,\n",
       " 'academy': 6.922918004572872,\n",
       " 'accents': 6.922918004572872,\n",
       " 'accessible': 6.922918004572872,\n",
       " 'acclaimed': 6.922918004572872,\n",
       " 'accolades': 6.922918004572872,\n",
       " 'accurate': 6.922918004572872,\n",
       " 'accurately': 6.922918004572872,\n",
       " 'achille': 6.922918004572872,\n",
       " 'ackerman': 6.922918004572872,\n",
       " 'actions': 6.922918004572872,\n",
       " 'adams': 6.922918004572872,\n",
       " 'add': 6.922918004572872,\n",
       " 'added': 6.922918004572872,\n",
       " 'admins': 6.922918004572872,\n",
       " 'admiration': 6.922918004572872,\n",
       " 'admitted': 6.922918004572872,\n",
       " 'adrift': 6.922918004572872,\n",
       " 'adventure': 6.922918004572872,\n",
       " 'aesthetically': 6.922918004572872,\n",
       " 'affected': 6.922918004572872,\n",
       " 'affleck': 6.922918004572872,\n",
       " 'afternoon': 6.922918004572872,\n",
       " 'aged': 6.922918004572872,\n",
       " 'ages': 6.922918004572872,\n",
       " 'agree': 6.922918004572872,\n",
       " 'agreed': 6.922918004572872,\n",
       " 'aimless': 6.922918004572872,\n",
       " 'aired': 6.922918004572872,\n",
       " 'akasha': 6.922918004572872,\n",
       " 'akin': 6.922918004572872,\n",
       " 'alert': 6.922918004572872,\n",
       " 'alike': 6.922918004572872,\n",
       " 'allison': 6.922918004572872,\n",
       " 'allow': 6.922918004572872,\n",
       " 'allowing': 6.922918004572872,\n",
       " 'alongside': 6.922918004572872,\n",
       " 'amateurish': 6.922918004572872,\n",
       " 'amaze': 6.922918004572872,\n",
       " 'amazed': 6.922918004572872,\n",
       " 'amazingly': 6.922918004572872,\n",
       " 'amusing': 6.922918004572872,\n",
       " 'amust': 6.922918004572872,\n",
       " 'anatomist': 6.922918004572872,\n",
       " 'angel': 6.922918004572872,\n",
       " 'angela': 6.922918004572872,\n",
       " 'angelina': 6.922918004572872}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idf_(dataset, vocab, max_f):\n",
    "    \n",
    "    unique_words = list(vocab.keys())\n",
    "    # intializing a dict to store idf values as values and subsequent words as keys\n",
    "    idf_val = {}\n",
    "    for word in unique_words: # for every word in list of unique words\n",
    "        doc_with_words = 0\n",
    "        for row in dataset: # for each sentence in dataset\n",
    "            if word in row.split():\n",
    "                doc_with_words += 1\n",
    "        # idf calculation \n",
    "        idf_val[word] = 1 + math.log((1 + len(dataset)) / float(1 + doc_with_words))\n",
    "        idf_val = Counter(idf_val)\n",
    "        # dictionary with top idf values as values and words as keys\n",
    "        idf_val = dict(idf_val.most_common(max_f))\n",
    "    return idf_val\n",
    "\n",
    "idf_(strings, vocabulary, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ```transform``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset, vocab, max_f ):\n",
    "    # intializing required list and dictionaries for calculating tf and idf\n",
    "    unique_words = list(vocab.keys())\n",
    "    idf_val = {}\n",
    "    tf_val = {}\n",
    "    tfidf_values = []\n",
    "    \n",
    "    \n",
    "    if isinstance(dataset, (list,)):  # check if its list type or not\n",
    "         # calculating idf\n",
    "        for word in unique_words: # for each word in list of unique words\n",
    "            doc_with_words = 0\n",
    "            for row in dataset: # for each sentence in dataset\n",
    "                if word in row.split():\n",
    "                    doc_with_words += 1\n",
    "             # idf calculation\n",
    "            idf_val[word] = 1 + math.log((1 + len(dataset)) / float(1 + doc_with_words))\n",
    "        idf_val = Counter(idf_val)\n",
    "        \n",
    "        # dictionary with top idf values as values and words as keys\n",
    "        idf_val = dict(idf_val.most_common(max_f))\n",
    "        # modified list of words with top idf values\n",
    "        mod_words = sorted(list(idf_val.keys()), reverse=True)\n",
    "        \n",
    "        # tf calculation\n",
    "        for word in mod_words: # for each word in list of modified unique words\n",
    "            sent_tf_vec = []\n",
    "            for row in dataset: # for each sentence in dataset\n",
    "                doc_freq = 0\n",
    "                for token in row.split(): # for each word in the sentence\n",
    "                    if word == token:\n",
    "                        doc_freq += 1\n",
    "                # calculating tf\n",
    "                tf = doc_freq / len(row.split())\n",
    "                sent_tf_vec.append(tf)\n",
    "            tf_val[word] = sent_tf_vec\n",
    "        # calculating tfidf \n",
    "        for token in tqdm(tf_val.keys()):  # for each word in tf_dictionary\n",
    "            tfidf_sentences = []\n",
    "            for tf_sentence in tf_val[token]:\n",
    "                tf_idf_score = tf_sentence * idf_val[token]\n",
    "                tfidf_sentences.append(tf_idf_score)\n",
    "            tfidf_values.append(tfidf_sentences)\n",
    "        \n",
    "        return csr_matrix(normalize(np.array(tfidf_values).T, norm = 'l2'))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 3562.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 50)\n"
     ]
    }
   ],
   "source": [
    "# shape of custom tfidf sparse matrix \n",
    "print(transform(strings, vocabulary, 50).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "746 rows and 50 columns as we have selected top 50 features according to idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# sparse output for first string\n",
    "print(transform(strings, vocabulary, 50)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 2482.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "# shape of sparse output for first string\n",
    "print(transform(strings, vocabulary, 50)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 row and 50 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 3072.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# dense output for first string\n",
    "print(transform(strings, vocabulary, 50)[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
